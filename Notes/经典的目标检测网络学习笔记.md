# 经典的目标检测网络学习笔记

记录经典的一些目标检测网络模型，即 RCNN 系列、SSD、YOLO 系列的学习。



------

## 1. SSD

参考文章：

- [目标检测|SSD原理与实现](https://zhuanlan.zhihu.com/p/33544892)



SSD，英文全名是 Single Shot MultiBox Detector，论文地址：

[SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325v5.pdf)

代码地址：

1. 官方地址：https://github.com/weiliu89/caffe
2. pytorch：https://github.com/amdegroot/ssd.pytorch
3. tensorflow：https://github.com/balancap/SSD-Tensorflow
4. mmdetection：https://github.com/open-mmlab/mmdetection



SSD 算法属于 one-stage 的方法，并且是多框检测。下面给出了几种不同检测算法的基本框架图，图片来自参考文章 1：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_1.png" style="zoom:50%;" />

根据上图，MultiBox 和 Faster-RCNN 都是 two-stage 的算法，而 YOLO 和 SSD 都是 one-stage 的算法，这两类算法的不同之处在于：

1. two-stage 一般是先通过一些启发式方法（比如 selective search）或者 CNN 网络（RPN）产生一些稀疏的候选框（这阶段就过滤了大量非目标物理的检测框），然后再对这些候选框进行分类和回归，其优势是**准确率高，但速度较慢**；
2. one-stage 方法的主要思路就是在图片不同位置均匀的进行密集抽样，主要是通过 anchor box 的方法，设置了不同尺度和长宽比的 anchors，然后利用 CNN 进行提取特征并进行分类和回归，直接一步完成，**优势就是速度快，但是缺点是训练很困难**，因为候选框过多，并且大量是负样本，即**正负样本极其不均衡**，导致模型准确率相比 two-stage 算法是稍差的；



而 SSD 和 YOLO 的区别在于：

1. Yolo 是在全连接层后做检测，而 SSD 则是采用卷积直接做检测；
2. SSD 采用不同尺度的特征图来做检测，大尺度的特征图（比较浅层的特征图）用来检测小物体，而小尺度特征图（深层的特征图）来检测大物体；
3. SSD 设置了不同的尺度和长宽比的先验框，Prior boxes，在 Faster-RCNN 中是叫做 Anchors

YOLO 存在的问题是难以检测小目标，并且定位不够准确，而 SSD 上述的改进则是为了避免 YOLO 的这些问题。



### 设计理念

SSD 的基本结构如下：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_2.png" style="zoom:50%;" />

SSD 的核心设计理念主要是以下 3 点：

1. 采用多尺度的特征图
2. 使用卷积进行检测
3. 设置先验框



#### 采用多尺度特征图

如下图所示，分别采用了 $8\times 8$ 和 $4\times 4$ 两种特征图，使用相同大小的检测框（也是$3\times 3$ 左右的大小），可以看到在大的特征图上可以检测到较小的目标，而小的特征图上可以检测大目标，这就是使用多尺度特征图的原因，不同尺度的特征图负责检测不同大小的目标，这也是解决了 yolo 不能检测到小目标的缺陷。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_3.png" style="zoom:50%;" />



#### 使用卷积进行检测

SSD 在最后直接采用卷积层来对不同的特征图提取检测结果，对于大小是 $m\times n\times p$ 的特征图，只需要采用 $3\times 3 \times p$ 这样比较小的卷积核就可以得到检测值，相比最后采用全连接层的 Yolo 可以进一步减少参数。



#### 设置先验框

在 Yolo 中每个单元预测多个边界框，但都是相对这个单元本身（正方块），但真实目标可能是多种不同形状，这需要 Yolo 在训练中自适应目标的形状。

而 SSD 则借鉴了 Faster R-CNN 中 anchor 的理念，也就是每个单元设置多个尺度或者长宽比不同的先验框（Prior boxes)，预测框以这些先验框为基准，这一定程度上减少了训练难度。如下图所示，一般每个单元会设置多个先验框，它们在尺度和长宽比上都存在差异，比如下图就是每个单元使用了 4 个不同的先验框，并且长宽比也不一样，有正方形，也有更宽的长方形或者更长的长方形。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_4.png" style="zoom:50%;" />

SSD 输出的检测值是包含两部分：

1. **每个类别的置信度或者评分**：SSD 是将背景也当做一个特殊的类别，所以假如检测的目标类别有 c 个，那么SSD 会输出 c+1 个置信度，第一个置信度就是指背景的置信度，也是不包含任何目标，然后将检测框的类别设置为置信度最高的那个类别；
2. **位置 location**：包含 4 个值，分别是 (cx, cy, w, h)，分别表示中心点坐标和宽高。

不过，注意这里预测框的location 其实是相对于先验框的转换值（论文里说是 offset）。假设先验框位置为 $d=(d_{cx},d_{cy},d_w,d_h)$ ，则其对应的标注框 $b=(b_{cx},b_{cy},b_w,b_h)$，则其预测框 l 其实是 b 相对于 d 的转换值，如下所示：
$$
l_{cx} = \frac{b_{cx}-d_{cx}}{d_w},l_{cy}=\frac{b_{cy}-d_{cy}}{d_h}\\
l_w=log(\frac{b_w}{d_w}),l_h=log(\frac{b_h}{d_h})
$$
习惯上来说，上述过程称为对标注框的编码（encode），而预测的时候，需要反过来，即进行解码（decode），从预测框 l 得到真实的标注框 b：
$$
b_{cx} = d_wl_{cx}+d_{cx},b_{cy}=d_hl_{cy}+d_{cy}\\
b_w=d_w exp(l_w),b_h=d_h exp(l_h)
$$
不过在 [caffe 版本的 SSD 代码 ](https://github.com/weiliu89/caffe/tree/ssd)实现中有个 trick，可以设置一个 `variance` 超参数来调整预测值，通过布尔型参数 `variance_encoded_in_target` 来控制两种模式，当它为 True 的时候，表示 `variance` 被包含在预测值里，即上述的情况；而如果是 False（大部分是这种方式，可能更易于训练），就需要手动设置这个超参数 `variance` ，用来对预测框 l 的四个值进行缩放，即如下进行解码：
$$
b_{cx} = d_w(variance[0] * l_{cx})+d_{cx},b_{cy}=d_h(variance[1] * l_{cy})+d_{cy}\\
b_w=d_w exp(variance[2] * l_w),b_h=d_h exp(variance[3] * l_h)
$$
综上所述，对于一个大小 $m\times n$ 的特征图，总共有 $mn$ 个单元（即每个像素点都是一个单元），每个单元设置的先验框数量记为 k，则每个单元需要 $(c+4)k$ 个预测值（每个预测框是 c 个置信度+4 个location 数值），所有单元就需要 $(c+4) kmn$ 个预测值，而因为 SSD 采用卷积做检测，所以需要 $(c+4)k$ 个卷积核完成这个特征图的检测过程。



### 网络结构

SSD 是采用 VGG16 作为 backbone，并在此基础增加更多的卷积层来完成检测，如下图所示，上面是 SSD 的网络结构，而下面是 Yolo 的网络结构。模型输入图片大小是 $300\times 300$ ，当然也可以是 $512\times 512$ ，只需要最后新增一个卷积层。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_5.png" style="zoom:50%;" />

SSD 将 VGG16 原本的全连接层 fc6 和 fc7 转换为 $3\times3 $ 的卷积层 conv6 和 $1\times 1$ 的卷积层 conv7，并将池化层 p5 从原来是 stride=2 的$2\times 2$ 改为 stride=1 的 $3\times 3$ （这里应该是不改变特征图大小），并且为了配合这种变化，采用了一种 Atrous Algothrim，操作上就是 conv6 使用了扩展卷积或者带孔卷积（Dilation Conv），它可以在不增加参数和模型复杂度的前提下指数级扩大卷积的视野，通过参数扩张率 dilation rate 来表示扩张的大小，如下图所示，a 图是普通的卷积核，其视野只是 $3\times 3$ ，而 b 图的扩展率是 2，视野就是 $7\times 7$ ，c 图的扩张率是 4，视野就变成了 $15\times 15$ ，但是缺点是特征会变得稀疏。这里 conv6 层采用了 $3\times 3$ 的卷积核大小，扩张率是 6 的扩展视野。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_6.png" style="zoom:50%;" />

然后还移除了 dropout 层和 fc8 层，并新增更多的卷积层。



这里 VGG16 的 Conv4_3 层将作为检测的第一个特征图，其特征图大小是 $38\times 38$，因为这层太靠前，其 norm 比较大，所以之后接了一个 L2 Normalizaton 层（参考 [ParseNet](https://arxiv.org/abs/1506.04579)），以保证和后面的检测层差异不会太大，该层的作用是对每个像素点在 channel 维度上做归一化，和 BN 不太一样，BN 是在 [batch, width, height] 三个维度上做归一化。

归一化后还会设置一个可训练的缩放变量 gamma，在 Tensorflow 中的实现如下：

```python
# l2norm (not bacth norm, spatial normalization)
def l2norm(x, scale, trainable=True, scope="L2Normalization"):
    n_channels = x.get_shape().as_list()[-1]
    l2_norm = tf.nn.l2_normalize(x, [3], epsilon=1e-12)
    with tf.variable_scope(scope):
        gamma = tf.get_variable("gamma", shape=[n_channels, ], dtype=tf.float32,
                                initializer=tf.constant_initializer(scale),
                                trainable=trainable)
        return l2_norm * gamma
```

SSD 还将提取 Conv7，Conv8_2，Conv9_2，Conv10_2，Conv11_2 作为检测所用的特征图，加上Conv4_3层，共提取了6个特征图，其大小分别是 $(38,38), (19,19),(10,10),(5,5),(3,3),(1,1)$ ，**但不同特征图会设置不同的先验框数量**，当然同个特征图的每个单元是相同数量的先验框。

先验框的设置包括两个方面，尺度和长宽比。首先，对于先验框的尺度，遵守一个线性递增规则：**随着特征图大小降低，先验框尺度线性增加**，如下所示：
$$
s_k = s_{min}+\frac{s_{max}-s_{min}}{m-1}(k-1),k\in[1,m]
$$
这里 m 表示特征图的个数，在 SSD 里是设置为 5，因为第一层(Conv4_3) 是单独设置，而 $s_k$ 表示先验框大小相对于原图的比例，$s_{min}，s_{max}$ 分别表示比例的最小值和最大值，论文里是分别设置 0.2 和 0.9。

对于第一个特征图（Conv4_3层的特征图），一般设置先验框的尺度比例为 $\frac{s_{min}}{2}= 0.1$ ，所以尺度就是原图大小 300 * 0.1 = 30。

对于后面的特征图，则按照上述公式线性增加，其操作步骤如下：

1. 首先一般会将尺度比例先扩大 100 倍，即 $\lfloor \frac{\lfloor s_{max}\times 100\rfloor - \lfloor s_{min}\times 100\rfloor}{m-1}\rfloor=17$，这样 5 个特征图的 $s_k$ 分别是 20，37，54，71 和 88
2. 再将它们除以 100 并乘以图片大小，得到的每个特征图的尺度就是 60，111，162，213，264，加上刚刚单独算的第一个特征图的尺度，**结果就是 30，60，111，162，213，264**



对于长宽比，一般选择 $a_r\in \{1,2,3,\frac{1}{2},\frac{1}{3}\}$ ，对于特定的长宽比，按如下公式计算先验框的宽度和高度：
$$
w_k^a = p_k\sqrt{a_r}, h_k^a = \frac{p_k}{\sqrt{a_r}}
$$
其中 $p_k$ 表示先验框实际尺度，默认情况下每个特征图都有一个 $a_r=1$ 且尺度是 $p_k$ 的先验框，除此之外还会设置一个尺度为 $p'_{k}=\sqrt{p_k p_{k+1}}$  且 $a_r=1$ 的先验框，即每个特征图就设置了两个长宽比都是 1，但是尺度不同的正方形先验框。

注意，最后一个特征图是要参考一个虚拟的$p_{m+1}=300\times 105 /100 = 315$ 来计算 $p'_{m}$ 。

因此，每个特征图一共有 6 个先验框 $\{1,2,3,\frac{1}{2},\frac{1}{3}, 1'\}$，不过注意，实际中，Conv4_3, Conv10_2 和 Conv11_2 仅使用 4 个先验框，它们不使用长宽比是 3 和 $\frac{1}{3}$ 的先验框。

每个单元先验框的中心点分布在各个单元的中心，即 $(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|}),i,j\in[0, |f_k|)$，其中 $|f_k|$ 表示特征图的大小。

上述是给出了先验框的设置，而下图给出了一个 $5\times 5$ 大小的特征图的检测过程，主要分为三个部分：

1. 生成先验框
2. 得到类别置信度
3. 得到边界框位置

其中第一步是上述介绍的，后面两步其实都是通过 $3\times 3$ 的卷积来完成，假设 $n_k$ 是该特征图采用的先验框数量，那么类别置信度需要的卷积核数量就是 $n_k \times c$ ，边界框位置需要的卷积核数量是 $n_k \times 4$ ，因为每个先验框都会预测一个边界框，因此 SSD300 一共可以预测 $38\times38\times4+19\times19\times6+10\times10\times6+5\times5\times6+3\times3\times4+1\times1\times4=8732$ 个边界框，**所以 SSD 本质上是密集采样**，或者说对于 one-stage 的检测算法，都属于密集采样。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_7.png" style="zoom:50%;" />





### 训练过程

#### 先验框匹配

训练过程首先是需要确定图片中的 ground truth（即标注框）和哪个先验框进行匹配，匹配的先验框对应的预测框将负责预测这个标注框。

**在 Yolo 中的规则是标注框的中心落在哪个单元，则该单元和其 IOU 最大的边界框负责预测它**。

但 SSD 采用不同的匹配原则，主要是两点：

1. 对每个标注框都找到和其 IOU 最大的先验框，该先验框和它匹配，保证每个标注框都有一个先验框匹配和对其进行预测，这种先验框也称为正样本，而如果没有匹配上标注框的先验框则是负样本。
2. 对于没有满足第一个原则的其他未匹配先验框，如果和某个标注框的 IOU 大于某个阈值（通常是 0.5），则该先验框也匹配该标注框，这表示一个标注框可以和多个先验框进行匹配，但是反过来是不可以的，不能一个先验框匹配多个标注框。



第一个原则会导致的一个问题就是负样本太多，因为标注框通常很少，而先验框非常多，这导致正负样本极其不平衡，所以也就有了第二个原则。

第二个原则在代码实现里是有多种实现方式的，比如：

1. [Tensorflow 版本](https://github.com/xiaohu2015/SSD-Tensorflow/blob/master/nets/ssd_common.py) 是只实施了第二个原则；
2. [Pytorch 版本 ](https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py)实施了两个原则

如下图示一个匹配示意图，绿色的GT 表示 ground truth，红色框是先验框，FP 表示负样本，TP 表示正样本。

<img src="images/SSD_8.png" style="zoom:50%;" />

尽管有上述两个原则，让一个标注框可以匹配多个先验框，但是由于先验框和标注框的数量差距还是很大，负样本还是会远多于正样本，所以 SSD 还采用了常见的一种处理样本不平衡的方法-- hard negative mining，即对负样本进行抽样，抽样是按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，然后选择误差较大的 top-k 作为训练时候的负样本，以保证正负样本比接近 1：3。



#### 损失函数

SSD 的损失函数是两部分组成，位置误差（locatization loss，los）和置信度误差（confidence loss，conf）的加权和，如下所示：
$$
L(x, c, l, g) = \frac{1}{N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g))
$$
N 表示先验框的正样本数量，$x^p_{ij}\in \{1,0\}$ 作为一个指示参数，当它为 1 的时候表示第 i 个先验框和第 j 个标注框匹配，并且标注框的类别是 p 。

c 为类别置信度预测值。l 是先验框对应的边界框的位置预测值，g 是标注框的位置参数，对于位置误差，采用的是 Smooth L1 Loss，定义如下：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_9.png" style="zoom:50%;" />

因为 $x^p_{ij}$ 的原因，所以位置误差仅针对正样本进行计算。需要注意的是，先要对标注框的 g 进行编码得到 $\hat{g}$ ，因为预测值 l 也是编码值，如果设置了 `variance_encoded_in_target=True` ，编码的时候就要加上 `variance` ：
$$
\hat{g}^{cx}_j = (g^{cx}_j - d^{cx}_i)/d^w_i/variance[0], \hat{g}^{cy}_j = (g^{cy}_j - d^{cy}_i)/d^h_i/variance[1]\\
\hat{g}^{w}_j = \log(g^{w}_j/d^w_i)/variance[2], \space \hat{g}^{h}_j = \log(g^{h}_j/d^h_i)/variance[3]
$$
对于位置误差，采用的是 softmax loss：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SSD_10.png" style="zoom:50%;" />

权重参数 $\alpha$ 通过交叉验证设置为 1



#### 数据扩增

采用的数据扩增方法主要有水平翻转(horizontal flip)、随机裁剪和颜色扭曲（random crop & color distortion），随机采集块（Randomly sample a patch）（获取小目标训练样本）。



### 预测过程

预测过程比较简单，对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值（如0.5）过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k（如400）个预测框。最后就是进行NMS算法，过滤掉那些重叠度较大的预测框。最后剩余的预测框就是检测结果了。

























